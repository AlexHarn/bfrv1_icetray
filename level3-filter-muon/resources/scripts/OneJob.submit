#############################################
#this shell script will call my python script for condor 
#  (condor is the batch scheduler for NPX3)
# Run using command $condor_submit condor.submit
# (if necess, kill with $ condor_rm jobnum
#      or $ condor_rm myusername
###############################################

# Jobname will name your output data, .out, .log, .err files
# This script name will show up on the cluster queue:
executable = $ENV(I3_SRC)/level3-filter-muon/python/level3_Master.py

# Get these vars from dag script, for each job
Jobname      = $(JOBNAME)
Infile       = $(INFILE)
Outfile      = $(OUTFILE)
HD5Outfile   = $(HD5OUTFILE)
RootOutfile  = $(ROOTOUTFILE)
Gcd          = $(GCD)
Logdir       = $(LOGDIR)

####################################
# Once you've set the stuff below here to your directories, 
# it probably won't change from job to job
###################################

#this is where the log, out, err files will live
output         = $(Logdir)/$(Cluster).out
error          = $(Logdir)/$(Cluster).err

log	= $(Logdir)/myMostRecent_condor.log

# This option takes all your env variables from your current session
# so there's no need to submit your env-shell.sh to the cluster
getenv         = true

#If you need to start not at job 0, talk to Nathan re: DagMan
#Process=

universe       = vanilla
notification   = complete

# now obsolete, default group has 48h limit
# use special accounting group for level3 processing
#+AccountingGroup="long.$ENV(USER)"

########################(drum roll..... SUBMIT!)####################
Arguments = -i $(Infile) -o $(Outfile) -g $(Gcd) --hd5output $(HD5Outfile) --rootoutput $(RootOutfile)
queue


